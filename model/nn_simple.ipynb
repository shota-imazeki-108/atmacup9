{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_category = [38, 110, 113, 114, 134, 171, 172, 173, 376, 435, 467, 537, 539, 629, 768]\n",
    "target_category_str = [str(col) for col in target_category]\n",
    "\n",
    "DEVICE= 'cpu'\n",
    "SEED = 42\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-8\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EARLY_STOPPING = 2\n",
    "# NUM_TARGETS = len(target_category_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../datasets/'\n",
    "train_df = pd.read_csv('../output/train_df.csv')\n",
    "test_X = pd.read_csv('../output/test_df.csv')\n",
    "\n",
    "train_Y = pd.read_csv('../output/train_Y.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_id_x</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>register_number</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>jp_holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>537_given</th>\n",
       "      <th>539_given</th>\n",
       "      <th>629_given</th>\n",
       "      <th>768_given</th>\n",
       "      <th>child_items</th>\n",
       "      <th>alone_items</th>\n",
       "      <th>cook_items</th>\n",
       "      <th>user_id_y</th>\n",
       "      <th>given_buy_num</th>\n",
       "      <th>avg_qoupon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>CN9sWHXp6RdCuyFkW5aemG</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>9</td>\n",
       "      <td>1005</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CN9sWHXp6RdCuyFkW5aemG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>Wi5hmLRCmUPXMRheu354dd</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>9</td>\n",
       "      <td>1010</td>\n",
       "      <td>147.0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Wi5hmLRCmUPXMRheu354dd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>kTFrFDLeaaggCoubWZJHpg</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>9</td>\n",
       "      <td>1010</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kTFrFDLeaaggCoubWZJHpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>exwdBc8tNJYAjhc4Gd6qtj</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>9</td>\n",
       "      <td>1011</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>exwdBc8tNJYAjhc4Gd6qtj</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>XUeiScqGsozKQFxcd3RDsD</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>9</td>\n",
       "      <td>1013</td>\n",
       "      <td>147.0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>XUeiScqGsozKQFxcd3RDsD</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id               user_id_x        date  hour  register_number  \\\n",
       "0         105  CN9sWHXp6RdCuyFkW5aemG  2019-02-14     9             1005   \n",
       "1         106  Wi5hmLRCmUPXMRheu354dd  2019-02-14     9             1010   \n",
       "2         107  kTFrFDLeaaggCoubWZJHpg  2019-02-14     9             1010   \n",
       "3         108  exwdBc8tNJYAjhc4Gd6qtj  2019-02-14     9             1011   \n",
       "4         109  XUeiScqGsozKQFxcd3RDsD  2019-02-14     9             1013   \n",
       "\n",
       "   time_elapsed  month  day  weekday  jp_holiday  ...  537_given  539_given  \\\n",
       "0         152.0      2   14        3           0  ...        0.0        0.0   \n",
       "1         147.0      2   14        3           0  ...        0.0        0.0   \n",
       "2         177.0      2   14        3           0  ...        0.0        0.0   \n",
       "3         247.0      2   14        3           0  ...        0.0        0.0   \n",
       "4         147.0      2   14        3           0  ...        0.0        0.0   \n",
       "\n",
       "   629_given  768_given  child_items  alone_items  cook_items  \\\n",
       "0        0.0        0.0          0.0          0.0         0.0   \n",
       "1        0.0        0.0          0.0          0.0         1.0   \n",
       "2        0.0        0.0          0.0          0.0         1.0   \n",
       "3        0.0        0.0          0.0          1.0         2.0   \n",
       "4        0.0        0.0          0.0          0.0         2.0   \n",
       "\n",
       "                user_id_y  given_buy_num  avg_qoupon  \n",
       "0  CN9sWHXp6RdCuyFkW5aemG            3.0         NaN  \n",
       "1  Wi5hmLRCmUPXMRheu354dd            2.0         NaN  \n",
       "2  kTFrFDLeaaggCoubWZJHpg            1.0         NaN  \n",
       "3  exwdBc8tNJYAjhc4Gd6qtj            4.0         1.0  \n",
       "4  XUeiScqGsozKQFxcd3RDsD            2.0         1.0  \n",
       "\n",
       "[5 rows x 520 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['age', 'gender', 'hour', 'weekday', 'jp_holiday', 'tgif', 'num_visit', 'month', 'day', 'register_number', 'max_time_avg', 'time_elapsed', 'hanakin', '38_avg', '110_avg', '113_avg', '114_avg', '134_avg', '171_avg', '172_avg', '173_avg', '376_avg', '435_avg', '467_avg', '537_avg', '539_avg', '629_avg', '768_avg', '38_given', '110_given', '113_given', '114_given', '134_given', '171_given', '172_given', '173_given', '376_given', '435_given', '467_given', '537_given', '539_given', '629_given', '768_given', '38_price_avg', '110_price_avg', '113_price_avg', '114_price_avg', '134_price_avg', '171_price_avg', '172_price_avg', '173_price_avg', '376_price_avg', '435_price_avg', '467_price_avg', '537_price_avg', '539_price_avg', '629_price_avg', '768_price_avg', 'child_items_sum', 'child_items_avg', 'child_items', '1_depart_avg', '2_depart_avg', '3_depart_avg', '4_depart_avg', '5_depart_avg', '7_depart_avg', '9_depart_avg', '10_depart_avg', '13_depart_avg', '14_depart_avg', '15_depart_avg', '16_depart_avg', '18_depart_avg', '19_depart_avg', '20_depart_avg', '21_depart_avg', '22_depart_avg', '23_depart_avg', '24_depart_avg', '25_depart_avg', '26_depart_avg', '27_depart_avg', '28_depart_avg', '29_depart_avg', '30_depart_avg', '32_depart_avg', '33_depart_avg', '34_depart_avg', '35_depart_avg', '36_depart_avg', '37_depart_avg', '38_depart_avg', '39_depart_avg', '40_depart_avg', '41_depart_avg', '46_depart_avg', '47_depart_avg', '49_depart_avg', '50_depart_avg', '58_depart_avg', '59_depart_avg', '60_depart_avg', '69_depart_avg', '70_depart_avg', '71_depart_avg', '72_depart_avg', '73_depart_avg', '74_depart_avg', '75_depart_avg', '77_depart_avg', '78_depart_avg', '79_depart_avg', '80_depart_avg', '81_depart_avg', '82_depart_avg', '83_depart_avg', '84_depart_avg', '87_depart_avg', '88_depart_avg', '89_depart_avg', '91_depart_avg', '92_depart_avg', '93_depart_avg', '94_depart_avg', '95_depart_avg', '96_depart_avg', '97_depart_avg', '98_depart_avg', '106_depart_avg', '107_depart_avg', '109_depart_avg', '117_depart_avg', '118_depart_avg', '121_depart_avg', '124_depart_avg', '131_depart_avg', '132_depart_avg', '133_depart_avg', '136_depart_avg', '137_depart_avg', '138_depart_avg', '141_depart_avg', '151_depart_avg', '152_depart_avg', '153_depart_avg', '154_depart_avg', '155_depart_avg', '156_depart_avg', '161_depart_avg', '162_depart_avg', '163_depart_avg', '165_depart_avg', '172_depart_avg', '173_depart_avg', '174_depart_avg', '178_depart_avg', '179_depart_avg', '182_depart_avg', '183_depart_avg', '185_depart_avg', '187_depart_avg', '194_depart_avg', '201_depart_avg', '202_depart_avg', '203_depart_avg', '206_depart_avg', '207_depart_avg', '210_depart_avg', '214_depart_avg', '215_depart_avg', '217_depart_avg', '219_depart_avg', '220_depart_avg', '221_depart_avg', '223_depart_avg', '224_depart_avg', '225_depart_avg', '226_depart_avg', '227_depart_avg', '228_depart_avg', '229_depart_avg', '230_depart_avg', '231_depart_avg', '232_depart_avg', '233_depart_avg', '234_depart_avg', 'cancel_items_sum', 'cancel_items_avg', 'buy_num_items_sum', 'buy_num_items_avg', 'given_buy_num', 'cancel10_items_sum', 'cancel10_items_avg', 'alone_items_sum', 'alone_items_avg', 'alone_items', 'cook_items_sum', 'cook_items_avg', 'cook_items', 'qoupon_avg', 'avg_qoupon', 'category_35_avg', 'category_37_avg', 'category_39_avg', 'category_40_avg', 'category_86_avg', 'category_111_avg', 'category_112_avg', 'category_135_avg', 'category_137_avg', 'category_141_avg', 'category_142_avg', 'category_143_avg', 'category_145_avg', 'category_148_avg', 'category_149_avg', 'category_150_avg', 'category_205_avg', 'category_206_avg', 'category_207_avg', 'category_208_avg', 'category_209_avg', 'category_210_avg', 'category_274_avg', 'category_275_avg', 'category_276_avg', 'category_289_avg', 'category_307_avg', 'category_310_avg', 'category_311_avg', 'category_312_avg', 'category_313_avg', 'category_316_avg', 'category_317_avg', 'category_319_avg', 'category_321_avg', 'category_328_avg', 'category_334_avg', 'category_340_avg', 'category_341_avg', 'category_342_avg', 'category_343_avg', 'category_344_avg', 'category_363_avg', 'category_365_avg', 'category_368_avg', 'category_370_avg', 'category_371_avg', 'category_372_avg', 'category_373_avg', 'category_374_avg', 'category_375_avg', 'category_376_avg', 'category_377_avg', 'category_378_avg', 'category_391_avg', 'category_392_avg', 'category_406_avg', 'category_407_avg', 'category_408_avg', 'category_410_avg', 'category_411_avg', 'category_414_avg', 'category_415_avg', 'category_416_avg', 'category_417_avg', 'category_420_avg', 'category_421_avg', 'category_422_avg', 'category_423_avg', 'category_424_avg', 'category_425_avg', 'category_426_avg', 'category_431_avg', 'category_432_avg', 'category_433_avg', 'category_436_avg', 'category_469_avg', 'category_470_avg', 'category_471_avg', 'category_472_avg', 'category_473_avg', 'category_474_avg', 'category_508_avg', 'category_509_avg', 'category_536_avg', 'category_538_avg', 'category_561_avg', 'category_562_avg', 'category_565_avg', 'category_566_avg', 'category_567_avg', 'category_568_avg', 'category_579_avg', 'category_587_avg', 'category_588_avg', 'category_589_avg', 'category_590_avg', 'category_591_avg', 'category_594_avg', 'category_602_avg', 'category_617_avg', 'category_619_avg', 'category_620_avg', 'category_621_avg', 'category_623_avg', 'category_628_avg', 'category_630_avg', 'category_631_avg', 'category_632_avg', 'category_633_avg', 'category_634_avg', 'category_636_avg', 'category_655_avg', 'category_665_avg', 'category_666_avg', 'category_669_avg', 'category_674_avg', 'category_679_avg', 'category_684_avg', 'category_708_avg', 'category_711_avg', 'category_716_avg', 'category_720_avg', 'category_724_avg', 'category_769_avg', 'category_770_avg', 'category_771_avg', 'similar_110_avg', 'similar_113_avg', 'similar_114_avg', 'similar_134_avg', 'similar_171_avg', 'similar_172_avg', 'similar_173_avg', 'similar_376_avg', 'similar_38_avg', 'similar_435_avg', 'similar_467_avg', 'similar_537_avg', 'similar_539_avg', 'similar_629_avg', 'similar_768_avg', 'category_35_given', 'category_37_given', 'category_39_given', 'category_40_given', 'category_86_given', 'category_111_given', 'category_112_given', 'category_135_given', 'category_136_given', 'category_137_given', 'category_141_given', 'category_142_given', 'category_143_given', 'category_145_given', 'category_148_given', 'category_149_given', 'category_150_given', 'category_205_given', 'category_206_given', 'category_207_given', 'category_208_given', 'category_209_given', 'category_210_given', 'category_274_given', 'category_275_given', 'category_276_given', 'category_289_given', 'category_294_given', 'category_295_given', 'category_299_given', 'category_307_given', 'category_310_given', 'category_311_given', 'category_312_given', 'category_313_given', 'category_314_given', 'category_315_given', 'category_316_given', 'category_317_given', 'category_319_given', 'category_321_given', 'category_328_given', 'category_330_given', 'category_331_given', 'category_334_given', 'category_340_given', 'category_341_given', 'category_342_given', 'category_343_given', 'category_344_given', 'category_346_given', 'category_363_given', 'category_365_given', 'category_366_given', 'category_367_given', 'category_368_given', 'category_370_given', 'category_371_given', 'category_372_given', 'category_373_given', 'category_374_given', 'category_375_given', 'category_376_given', 'category_377_given', 'category_378_given', 'category_391_given', 'category_392_given', 'category_393_given', 'category_406_given', 'category_407_given', 'category_408_given', 'category_410_given', 'category_411_given', 'category_414_given', 'category_415_given', 'category_416_given', 'category_417_given', 'category_418_given', 'category_420_given', 'category_421_given', 'category_422_given', 'category_423_given', 'category_424_given', 'category_425_given', 'category_426_given', 'category_430_given', 'category_431_given', 'category_432_given', 'category_433_given', 'category_434_given', 'category_436_given', 'category_468_given', 'category_469_given', 'category_470_given', 'category_471_given', 'category_472_given', 'category_473_given', 'category_474_given', 'category_508_given', 'category_509_given', 'category_510_given', 'category_536_given', 'category_538_given', 'category_546_given', 'category_547_given', 'category_548_given', 'category_561_given', 'category_562_given', 'category_565_given', 'category_566_given', 'category_567_given', 'category_568_given', 'category_569_given', 'category_579_given', 'category_587_given', 'category_588_given', 'category_589_given', 'category_590_given', 'category_591_given', 'category_594_given', 'category_602_given', 'category_613_given', 'category_615_given', 'category_616_given', 'category_617_given', 'category_619_given', 'category_620_given', 'category_623_given', 'category_628_given', 'category_630_given', 'category_631_given', 'category_632_given', 'category_633_given', 'category_634_given', 'category_635_given', 'category_636_given', 'category_655_given', 'category_662_given', 'category_665_given', 'category_666_given', 'category_667_given', 'category_669_given', 'category_674_given', 'category_679_given', 'category_684_given', 'category_696_given', 'category_708_given', 'category_711_given', 'category_716_given', 'category_720_given', 'category_724_given', 'category_726_given', 'category_756_given', 'category_769_given', 'category_770_given', 'category_771_given', 'similar_110_given', 'similar_113_given', 'similar_114_given', 'similar_134_given', 'similar_171_given', 'similar_172_given', 'similar_173_given', 'similar_376_given', 'similar_38_given', 'similar_435_given', 'similar_467_given', 'similar_537_given', 'similar_539_given', 'similar_629_given', 'similar_768_given']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>jp_holiday</th>\n",
       "      <th>tgif</th>\n",
       "      <th>num_visit</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>similar_172_given</th>\n",
       "      <th>similar_173_given</th>\n",
       "      <th>similar_376_given</th>\n",
       "      <th>similar_38_given</th>\n",
       "      <th>similar_435_given</th>\n",
       "      <th>similar_467_given</th>\n",
       "      <th>similar_537_given</th>\n",
       "      <th>similar_539_given</th>\n",
       "      <th>similar_629_given</th>\n",
       "      <th>similar_768_given</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id   age  gender  hour  weekday  jp_holiday  tgif  num_visit  \\\n",
       "0         105  40.0     0.0     9        3           0     0          0   \n",
       "1         106  60.0     1.0     9        3           0     0          0   \n",
       "2         107  30.0     0.0     9        3           0     0          0   \n",
       "3         108  60.0     0.0     9        3           0     0          0   \n",
       "4         109  70.0     0.0     9        3           0     0          0   \n",
       "\n",
       "   month  day  ...  similar_172_given  similar_173_given  similar_376_given  \\\n",
       "0      2   14  ...                0.0                0.0                0.0   \n",
       "1      2   14  ...                0.0                0.0                0.0   \n",
       "2      2   14  ...                0.0                0.0                0.0   \n",
       "3      2   14  ...                0.0                0.0                1.0   \n",
       "4      2   14  ...                0.0                0.0                0.0   \n",
       "\n",
       "   similar_38_given  similar_435_given  similar_467_given  similar_537_given  \\\n",
       "0               0.0                0.0                0.0                0.0   \n",
       "1               0.0                0.0                0.0                0.0   \n",
       "2               0.0                0.0                0.0                0.0   \n",
       "3               0.0                0.0                0.0                0.0   \n",
       "4               0.0                0.0                0.0                0.0   \n",
       "\n",
       "   similar_539_given  similar_629_given  similar_768_given  \n",
       "0                0.0                0.0                0.0  \n",
       "1                0.0                0.0                0.0  \n",
       "2                0.0                0.0                0.0  \n",
       "3                0.0                0.0                0.0  \n",
       "4                0.0                0.0                0.0  \n",
       "\n",
       "[5 rows x 516 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_cols = ['age', 'gender', 'hour', 'weekday', 'jp_holiday', 'tgif', 'num_visit', 'day', 'register_number', 'max_time_avg']\n",
    "# categorical_cols = ['gender', 'weekday', 'jp_holiday', 'register_number']\n",
    "categorical_cols =  ['gender', 'weekday', 'jp_holiday', 'tgif', 'month', 'day', 'register_number', 'hanakin']\n",
    "numerical_cols = list(set(feature_cols) - set(categorical_cols))\n",
    "# print(numerical_cols)\n",
    "# train_df__ = pd.read_csv('../output/train_df.csv')\n",
    "train_df = train_df[['session_id'] + feature_cols]\n",
    "test_X['session_id'] = None\n",
    "test_X = test_X[['session_id'] + feature_cols]\n",
    "\n",
    "train_df.fillna(-1, inplace=True) # ラグとかでないのばかりだから、-1でいいはず...\n",
    "test_X.fillna(-1, inplace=True) # ラグとかでないのばかりだから、-1でいいはず...\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>jp_holiday</th>\n",
       "      <th>tgif</th>\n",
       "      <th>num_visit</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>similar_173_given</th>\n",
       "      <th>similar_376_given</th>\n",
       "      <th>similar_38_given</th>\n",
       "      <th>similar_435_given</th>\n",
       "      <th>similar_467_given</th>\n",
       "      <th>similar_537_given</th>\n",
       "      <th>similar_539_given</th>\n",
       "      <th>similar_629_given</th>\n",
       "      <th>similar_768_given</th>\n",
       "      <th>flg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 517 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id   age  gender  hour  weekday  jp_holiday  tgif  num_visit  \\\n",
       "0         105  40.0     0.0     9        3           0     0          0   \n",
       "1         106  60.0     1.0     9        3           0     0          0   \n",
       "2         107  30.0     0.0     9        3           0     0          0   \n",
       "3         108  60.0     0.0     9        3           0     0          0   \n",
       "4         109  70.0     0.0     9        3           0     0          0   \n",
       "\n",
       "   month  day  ...  similar_173_given  similar_376_given  similar_38_given  \\\n",
       "0      2   14  ...                0.0                0.0               0.0   \n",
       "1      2   14  ...                0.0                0.0               0.0   \n",
       "2      2   14  ...                0.0                0.0               0.0   \n",
       "3      2   14  ...                0.0                1.0               0.0   \n",
       "4      2   14  ...                0.0                0.0               0.0   \n",
       "\n",
       "   similar_435_given  similar_467_given  similar_537_given  similar_539_given  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "1                0.0                0.0                0.0                0.0   \n",
       "2                0.0                0.0                0.0                0.0   \n",
       "3                0.0                0.0                0.0                0.0   \n",
       "4                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   similar_629_given  similar_768_given  flg  \n",
       "0                0.0                0.0    0  \n",
       "1                0.0                0.0    0  \n",
       "2                0.0                0.0    0  \n",
       "3                0.0                0.0    0  \n",
       "4                0.0                0.0    0  \n",
       "\n",
       "[5 rows x 517 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['flg'] = 0\n",
    "test_X['flg'] = 1\n",
    "feature_df = pd.concat([train_df, test_X])\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>jp_holiday</th>\n",
       "      <th>tgif</th>\n",
       "      <th>num_visit</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>similar_173_given</th>\n",
       "      <th>similar_376_given</th>\n",
       "      <th>similar_38_given</th>\n",
       "      <th>similar_435_given</th>\n",
       "      <th>similar_467_given</th>\n",
       "      <th>similar_537_given</th>\n",
       "      <th>similar_539_given</th>\n",
       "      <th>similar_629_given</th>\n",
       "      <th>similar_768_given</th>\n",
       "      <th>flg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>-0.360416</td>\n",
       "      <td>-0.358481</td>\n",
       "      <td>-1.7209</td>\n",
       "      <td>-0.106536</td>\n",
       "      <td>-0.812772</td>\n",
       "      <td>-0.414402</td>\n",
       "      <td>-0.834219</td>\n",
       "      <td>-1.358742</td>\n",
       "      <td>-0.221552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202179</td>\n",
       "      <td>-0.313051</td>\n",
       "      <td>-0.181585</td>\n",
       "      <td>-0.197462</td>\n",
       "      <td>-0.159828</td>\n",
       "      <td>-0.263252</td>\n",
       "      <td>-0.217918</td>\n",
       "      <td>-0.118546</td>\n",
       "      <td>-0.217622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>1.006240</td>\n",
       "      <td>0.327001</td>\n",
       "      <td>-1.7209</td>\n",
       "      <td>-0.106536</td>\n",
       "      <td>-0.812772</td>\n",
       "      <td>-0.414402</td>\n",
       "      <td>-0.834219</td>\n",
       "      <td>-1.358742</td>\n",
       "      <td>-0.221552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202179</td>\n",
       "      <td>-0.313051</td>\n",
       "      <td>-0.181585</td>\n",
       "      <td>-0.197462</td>\n",
       "      <td>-0.159828</td>\n",
       "      <td>-0.263252</td>\n",
       "      <td>-0.217918</td>\n",
       "      <td>-0.118546</td>\n",
       "      <td>-0.217622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>-1.043744</td>\n",
       "      <td>-0.358481</td>\n",
       "      <td>-1.7209</td>\n",
       "      <td>-0.106536</td>\n",
       "      <td>-0.812772</td>\n",
       "      <td>-0.414402</td>\n",
       "      <td>-0.834219</td>\n",
       "      <td>-1.358742</td>\n",
       "      <td>-0.221552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202179</td>\n",
       "      <td>-0.313051</td>\n",
       "      <td>-0.181585</td>\n",
       "      <td>-0.197462</td>\n",
       "      <td>-0.159828</td>\n",
       "      <td>-0.263252</td>\n",
       "      <td>-0.217918</td>\n",
       "      <td>-0.118546</td>\n",
       "      <td>-0.217622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>1.006240</td>\n",
       "      <td>-0.358481</td>\n",
       "      <td>-1.7209</td>\n",
       "      <td>-0.106536</td>\n",
       "      <td>-0.812772</td>\n",
       "      <td>-0.414402</td>\n",
       "      <td>-0.834219</td>\n",
       "      <td>-1.358742</td>\n",
       "      <td>-0.221552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202179</td>\n",
       "      <td>1.804079</td>\n",
       "      <td>-0.181585</td>\n",
       "      <td>-0.197462</td>\n",
       "      <td>-0.159828</td>\n",
       "      <td>-0.263252</td>\n",
       "      <td>-0.217918</td>\n",
       "      <td>-0.118546</td>\n",
       "      <td>-0.217622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>1.689568</td>\n",
       "      <td>-0.358481</td>\n",
       "      <td>-1.7209</td>\n",
       "      <td>-0.106536</td>\n",
       "      <td>-0.812772</td>\n",
       "      <td>-0.414402</td>\n",
       "      <td>-0.834219</td>\n",
       "      <td>-1.358742</td>\n",
       "      <td>-0.221552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202179</td>\n",
       "      <td>-0.313051</td>\n",
       "      <td>-0.181585</td>\n",
       "      <td>-0.197462</td>\n",
       "      <td>-0.159828</td>\n",
       "      <td>-0.263252</td>\n",
       "      <td>-0.217918</td>\n",
       "      <td>-0.118546</td>\n",
       "      <td>-0.217622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 517 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id       age    gender    hour   weekday  jp_holiday      tgif  \\\n",
       "0         105 -0.360416 -0.358481 -1.7209 -0.106536   -0.812772 -0.414402   \n",
       "1         106  1.006240  0.327001 -1.7209 -0.106536   -0.812772 -0.414402   \n",
       "2         107 -1.043744 -0.358481 -1.7209 -0.106536   -0.812772 -0.414402   \n",
       "3         108  1.006240 -0.358481 -1.7209 -0.106536   -0.812772 -0.414402   \n",
       "4         109  1.689568 -0.358481 -1.7209 -0.106536   -0.812772 -0.414402   \n",
       "\n",
       "   num_visit     month       day  ...  similar_173_given  similar_376_given  \\\n",
       "0  -0.834219 -1.358742 -0.221552  ...          -0.202179          -0.313051   \n",
       "1  -0.834219 -1.358742 -0.221552  ...          -0.202179          -0.313051   \n",
       "2  -0.834219 -1.358742 -0.221552  ...          -0.202179          -0.313051   \n",
       "3  -0.834219 -1.358742 -0.221552  ...          -0.202179           1.804079   \n",
       "4  -0.834219 -1.358742 -0.221552  ...          -0.202179          -0.313051   \n",
       "\n",
       "   similar_38_given  similar_435_given  similar_467_given  similar_537_given  \\\n",
       "0         -0.181585          -0.197462          -0.159828          -0.263252   \n",
       "1         -0.181585          -0.197462          -0.159828          -0.263252   \n",
       "2         -0.181585          -0.197462          -0.159828          -0.263252   \n",
       "3         -0.181585          -0.197462          -0.159828          -0.263252   \n",
       "4         -0.181585          -0.197462          -0.159828          -0.263252   \n",
       "\n",
       "   similar_539_given  similar_629_given  similar_768_given  flg  \n",
       "0          -0.217918          -0.118546          -0.217622    0  \n",
       "1          -0.217918          -0.118546          -0.217622    0  \n",
       "2          -0.217918          -0.118546          -0.217622    0  \n",
       "3          -0.217918          -0.118546          -0.217622    0  \n",
       "4          -0.217918          -0.118546          -0.217622    0  \n",
       "\n",
       "[5 rows x 517 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "feature_df[feature_cols] = scaler.fit_transform(feature_df[feature_cols])\n",
    "feature_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = feature_df[feature_df['flg']==0]\n",
    "test_X = feature_df[feature_df['flg']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features,\n",
    "                 out_features,\n",
    "                 bias=True, p=0.3):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features,\n",
    "                               out_features,\n",
    "                               bias)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.ln2 = nn.Linear(1024, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.ln2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def make_model(input_dim):\n",
    "    return nn.Sequential(\n",
    "                    CustomLinear(input_dim, 1024),\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(target_col):\n",
    "    print('*'*100)\n",
    "    print('train:', target_col)\n",
    "\n",
    "    true_ids = list(train_Y[train_Y[target_col]==1].session_id.unique())\n",
    "    false_ids = list(train_Y[train_Y[target_col]!=1].session_id.unique())\n",
    "    print(target_col, len(true_ids), len(false_ids))\n",
    "\n",
    "    false_ids = random.sample(false_ids, len(true_ids)) # down samplling\n",
    "    sampling_ids = true_ids + false_ids\n",
    "    sampling_ids = sorted(sampling_ids)\n",
    "\n",
    "    train_idx = sampling_ids[: int(len(sampling_ids)*0.8)]\n",
    "    val_idx = sampling_ids[int(len(sampling_ids)*0.8): ]\n",
    "\n",
    "    assert set(train_idx) & set(val_idx) == set()\n",
    "    train_X = torch.tensor(train_df[train_df.session_id.isin(train_idx)][feature_cols].values, dtype=torch.float32).to(DEVICE)\n",
    "    train_y = torch.tensor(train_Y[train_Y.session_id.isin(train_idx)][target_col].values, dtype=torch.float32).unsqueeze(-1).to(DEVICE)\n",
    "\n",
    "    valid_X = torch.tensor(train_df[train_df.session_id.isin(val_idx)][feature_cols].values, dtype=torch.float32).to(DEVICE)\n",
    "    valid_y = torch.tensor(train_Y[train_Y.session_id.isin(val_idx)][target_col].values, dtype=torch.float32).unsqueeze(-1).to(DEVICE)\n",
    "    print('train & valid: ', len(train_X), len(valid_X))\n",
    "\n",
    "    model = make_model(len(feature_cols))\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "#     loss_fn = nn.CrossEntropyLoss() # 微妙？という話も聞く\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Dataset\n",
    "    train = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "    valid = torch.utils.data.TensorDataset(valid_X, valid_y)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    best_score = 0\n",
    "    stopping_cnt = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        # train\n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((valid_X.size(0)))\n",
    "        avg_val_loss = 0.\n",
    "        len_pred = 0\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            if len(y_batch) == BATCH_SIZE:\n",
    "                valid_preds[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = y_pred.cpu().numpy()[:, 0]\n",
    "            else:\n",
    "                valid_preds[i * BATCH_SIZE: i * BATCH_SIZE + len(y_batch)] = y_pred.cpu().numpy()[:, 0]\n",
    "        valid_preds = np.nan_to_num(valid_preds)\n",
    "        score = roc_auc_score(valid_y.numpy()[:, 0], valid_preds)\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS} \\t loss={avg_loss} \\t val_loss={avg_val_loss}, auc: {score}')\n",
    "\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(), f'../nn_models/best_model_{target_col}.pth')\n",
    "            stopping_cnt = 0\n",
    "        else:\n",
    "            stopping_cnt += 1\n",
    "            if stopping_cnt > EARLY_STOPPING:\n",
    "                print(f'EARLY STOPPING!!, best AUC: {best_score}')\n",
    "                break\n",
    "    return best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "train: 38\n",
      "38 30544 374281\n",
      "train & valid:  48870 12218\n",
      "Epoch 1/20 \t loss=166.754566691933 \t val_loss=159.06917206446332, auc: 0.7567192250614924\n",
      "Epoch 2/20 \t loss=162.6798513522323 \t val_loss=157.56610393524167, auc: 0.7596484076298446\n",
      "Epoch 3/20 \t loss=161.8697434670014 \t val_loss=156.41956202189127, auc: 0.7638287852768921\n",
      "Epoch 4/20 \t loss=161.32947496344275 \t val_loss=156.1789566675822, auc: 0.766008427893322\n",
      "Epoch 5/20 \t loss=160.86368468918724 \t val_loss=156.10998058319095, auc: 0.7665822482258559\n",
      "Epoch 6/20 \t loss=160.345528088315 \t val_loss=156.47406117121383, auc: 0.7666545360457098\n",
      "Epoch 7/20 \t loss=159.82651451869793 \t val_loss=156.22240034739175, auc: 0.765393749820166\n",
      "Epoch 8/20 \t loss=159.32207812563905 \t val_loss=156.66006708145144, auc: 0.7604988181091316\n",
      "Epoch 9/20 \t loss=158.7906618767384 \t val_loss=156.3724589347839, auc: 0.7635728079215898\n",
      "EARLY STOPPING!!, best AUC: 0.7666545360457098\n",
      "****************************************************************************************************\n",
      "train: 110\n",
      "110 5730 399095\n",
      "train & valid:  9168 2292\n",
      "Epoch 1/20 \t loss=171.33959325154623 \t val_loss=173.0305904812283, auc: 0.693126046194409\n",
      "Epoch 2/20 \t loss=165.22045728895404 \t val_loss=171.45484076605902, auc: 0.7064950363193333\n",
      "Epoch 3/20 \t loss=162.75197643703888 \t val_loss=168.95528496636285, auc: 0.7158695533494752\n",
      "Epoch 4/20 \t loss=161.10265180799695 \t val_loss=168.74101596408417, auc: 0.7256304462660712\n",
      "Epoch 5/20 \t loss=159.77129406399197 \t val_loss=168.98453436957465, auc: 0.7323603068555\n",
      "Epoch 6/20 \t loss=158.5923114352756 \t val_loss=167.25351799858942, auc: 0.7349000074584002\n",
      "Epoch 7/20 \t loss=157.20696089002823 \t val_loss=167.72088453504773, auc: 0.7351564380218478\n",
      "Epoch 8/20 \t loss=156.1412853664822 \t val_loss=169.1727803548177, auc: 0.7295441841009975\n",
      "Epoch 9/20 \t loss=155.22250790066184 \t val_loss=167.59986538357202, auc: 0.7354862992264024\n",
      "Epoch 10/20 \t loss=154.21630308363171 \t val_loss=166.9185994466146, auc: 0.7333398946750866\n",
      "Epoch 11/20 \t loss=153.34471744961206 \t val_loss=168.91448805067273, auc: 0.731288065713888\n",
      "Epoch 12/20 \t loss=152.46217007107205 \t val_loss=169.6040530734592, auc: 0.7322315148933488\n",
      "EARLY STOPPING!!, best AUC: 0.7354862992264024\n",
      "****************************************************************************************************\n",
      "train: 113\n",
      "113 14570 390255\n",
      "train & valid:  23312 5828\n",
      "Epoch 1/20 \t loss=168.03453675560326 \t val_loss=162.03678031589675, auc: 0.7339861451683174\n",
      "Epoch 2/20 \t loss=162.91245058308476 \t val_loss=161.32570216966715, auc: 0.7443011532150888\n",
      "Epoch 3/20 \t loss=161.60236636452052 \t val_loss=160.81411577307665, auc: 0.7471957943026236\n",
      "Epoch 4/20 \t loss=160.60189768542415 \t val_loss=160.97922681725544, auc: 0.7426205253082686\n",
      "Epoch 5/20 \t loss=160.11774834342629 \t val_loss=160.6090200672979, auc: 0.7468977679140314\n",
      "Epoch 6/20 \t loss=159.60396413181138 \t val_loss=160.89952054231063, auc: 0.7432040594418268\n",
      "EARLY STOPPING!!, best AUC: 0.7471957943026236\n",
      "****************************************************************************************************\n",
      "train: 114\n",
      "114 79493 325332\n",
      "train & valid:  127188 31798\n",
      "Epoch 1/20 \t loss=168.7486798844826 \t val_loss=164.92311904907228, auc: 0.7217061541054671\n",
      "Epoch 2/20 \t loss=167.48142514142464 \t val_loss=164.8934080810547, auc: 0.7231690450133061\n",
      "Epoch 3/20 \t loss=167.127527187048 \t val_loss=164.83240090942388, auc: 0.7226810348716025\n",
      "Epoch 4/20 \t loss=166.62055920115418 \t val_loss=164.921635559082, auc: 0.7247508789713957\n",
      "Epoch 5/20 \t loss=166.15544693330872 \t val_loss=164.9808836975098, auc: 0.7232273958152811\n",
      "Epoch 6/20 \t loss=165.5597893697634 \t val_loss=165.02537695312498, auc: 0.7233265259046303\n",
      "Epoch 7/20 \t loss=164.92758289237358 \t val_loss=165.13131158447263, auc: 0.7227362500227508\n",
      "EARLY STOPPING!!, best AUC: 0.7247508789713957\n",
      "****************************************************************************************************\n",
      "train: 134\n",
      "134 54040 350785\n",
      "train & valid:  86464 21616\n",
      "Epoch 1/20 \t loss=169.77697356636014 \t val_loss=166.11334066951972, auc: 0.7130030727069245\n",
      "Epoch 2/20 \t loss=168.38794708251956 \t val_loss=166.01120524686925, auc: 0.7169543734341958\n",
      "Epoch 3/20 \t loss=167.8981115352472 \t val_loss=165.87906179989082, auc: 0.7161360530429306\n",
      "Epoch 4/20 \t loss=167.4120248196392 \t val_loss=166.24635566262637, auc: 0.7156370477532614\n",
      "Epoch 5/20 \t loss=166.85488498845743 \t val_loss=166.04627532958983, auc: 0.7159506227337917\n",
      "EARLY STOPPING!!, best AUC: 0.7169543734341958\n",
      "****************************************************************************************************\n",
      "train: 171\n",
      "171 34143 370682\n",
      "train & valid:  54628 13658\n",
      "Epoch 1/20 \t loss=160.4628325132567 \t val_loss=155.06138066892274, auc: 0.8016299598197468\n",
      "Epoch 2/20 \t loss=157.0663790479998 \t val_loss=154.75970882839627, auc: 0.8071043840979462\n",
      "Epoch 3/20 \t loss=156.56316297299398 \t val_loss=154.40777347705978, auc: 0.8088241429717866\n",
      "Epoch 4/20 \t loss=156.02302982651193 \t val_loss=154.40626653035483, auc: 0.8088606819860535\n",
      "Epoch 5/20 \t loss=155.7293651616463 \t val_loss=154.5089099318893, auc: 0.804470945978661\n",
      "Epoch 6/20 \t loss=155.31966844006126 \t val_loss=154.56231597617824, auc: 0.8080426265750086\n",
      "Epoch 7/20 \t loss=154.94570790941466 \t val_loss=154.27673601221153, auc: 0.8086210912161456\n",
      "EARLY STOPPING!!, best AUC: 0.8088606819860535\n",
      "****************************************************************************************************\n",
      "train: 172\n",
      "172 7488 397337\n",
      "train & valid:  11980 2996\n",
      "Epoch 1/20 \t loss=169.3765349692487 \t val_loss=157.92738278706867, auc: 0.7576622686699092\n",
      "Epoch 2/20 \t loss=160.08319156727893 \t val_loss=154.5537255605062, auc: 0.7913961165785356\n",
      "Epoch 3/20 \t loss=157.13785893866353 \t val_loss=153.0034777323405, auc: 0.8014214063619541\n",
      "Epoch 4/20 \t loss=155.46399023177779 \t val_loss=152.0580902099609, auc: 0.8080434011075719\n",
      "Epoch 5/20 \t loss=154.45649134859127 \t val_loss=151.37119674682614, auc: 0.8095605649018878\n",
      "Epoch 6/20 \t loss=153.66527654769575 \t val_loss=152.30478731791177, auc: 0.8026035307484034\n",
      "Epoch 7/20 \t loss=153.27384786402922 \t val_loss=151.5189151763916, auc: 0.8104138997828159\n",
      "Epoch 8/20 \t loss=152.6065363782517 \t val_loss=150.8107121785482, auc: 0.808553214570308\n",
      "Epoch 9/20 \t loss=151.67126481076502 \t val_loss=150.84054946899414, auc: 0.8091514195090701\n",
      "Epoch 10/20 \t loss=151.6675494579559 \t val_loss=150.8080062866211, auc: 0.8054246920249014\n",
      "EARLY STOPPING!!, best AUC: 0.8104138997828159\n",
      "****************************************************************************************************\n",
      "train: 173\n",
      "173 39113 365712\n",
      "train & valid:  62580 15646\n",
      "Epoch 1/20 \t loss=160.01715994075852 \t val_loss=153.87687587738037, auc: 0.8159643253709443\n",
      "Epoch 2/20 \t loss=157.229815175582 \t val_loss=153.5266403690461, auc: 0.8119536644837487\n",
      "Epoch 3/20 \t loss=156.74098523198333 \t val_loss=153.22480586267278, auc: 0.8168964171579847\n",
      "Epoch 4/20 \t loss=156.42001084308234 \t val_loss=153.38648946823614, auc: 0.8135157846816709\n",
      "Epoch 5/20 \t loss=156.17011728092123 \t val_loss=153.1857514535226, auc: 0.8138122965344186\n",
      "Epoch 6/20 \t loss=155.79738682338177 \t val_loss=153.31274604797372, auc: 0.8148598011409975\n",
      "EARLY STOPPING!!, best AUC: 0.8168964171579847\n",
      "****************************************************************************************************\n",
      "train: 376\n",
      "376 34444 370381\n",
      "train & valid:  55110 13778\n",
      "Epoch 1/20 \t loss=168.36278844762742 \t val_loss=164.87021608705868, auc: 0.7237299886390377\n",
      "Epoch 2/20 \t loss=166.3914373009292 \t val_loss=164.35251645688655, auc: 0.7280217987937306\n",
      "Epoch 3/20 \t loss=165.8479413632992 \t val_loss=165.02433183458115, auc: 0.7234994167701974\n",
      "Epoch 4/20 \t loss=165.41649233853386 \t val_loss=164.65615618670424, auc: 0.725268336314848\n",
      "Epoch 5/20 \t loss=164.9374481836955 \t val_loss=164.9174491034614, auc: 0.7275059904405978\n",
      "EARLY STOPPING!!, best AUC: 0.7280217987937306\n",
      "****************************************************************************************************\n",
      "train: 435\n",
      "435 18991 385834\n",
      "train & valid:  30385 7597\n",
      "Epoch 1/20 \t loss=164.51313788149534 \t val_loss=154.32195103963213, auc: 0.797069145407825\n",
      "Epoch 2/20 \t loss=158.08986324021788 \t val_loss=153.22737859090168, auc: 0.8043090629546159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 \t loss=157.1048616040654 \t val_loss=153.11648890177415, auc: 0.8068537321653155\n",
      "Epoch 4/20 \t loss=156.6934151529265 \t val_loss=152.73254343668623, auc: 0.8074797583367559\n",
      "Epoch 5/20 \t loss=155.9322637349618 \t val_loss=152.08301137288413, auc: 0.8093857011055897\n",
      "Epoch 6/20 \t loss=155.48268883969604 \t val_loss=152.41350758870442, auc: 0.8102325593015879\n",
      "Epoch 7/20 \t loss=155.0648562647715 \t val_loss=152.8485740661621, auc: 0.8076187673075884\n",
      "Epoch 8/20 \t loss=154.7069162961816 \t val_loss=152.43462575276695, auc: 0.8088957343886263\n",
      "Epoch 9/20 \t loss=154.1392793414973 \t val_loss=152.64034779866537, auc: 0.806188633577211\n",
      "EARLY STOPPING!!, best AUC: 0.8102325593015879\n",
      "****************************************************************************************************\n",
      "train: 467\n",
      "467 9108 395717\n",
      "train & valid:  14572 3644\n",
      "Epoch 1/20 \t loss=168.2389017740885 \t val_loss=152.69843597412108, auc: 0.7516253482564694\n",
      "Epoch 2/20 \t loss=161.37292774936608 \t val_loss=150.40331013997397, auc: 0.7663341027450832\n",
      "Epoch 3/20 \t loss=159.2218432510108 \t val_loss=149.6753443400065, auc: 0.7753647741438895\n",
      "Epoch 4/20 \t loss=157.75723721688254 \t val_loss=148.95690765380857, auc: 0.7772880007194499\n",
      "Epoch 5/20 \t loss=157.00395175866913 \t val_loss=148.53123830159507, auc: 0.7818616251877732\n",
      "Epoch 6/20 \t loss=156.098209314179 \t val_loss=147.90434773763025, auc: 0.7845024849183488\n",
      "Epoch 7/20 \t loss=154.99014924701882 \t val_loss=147.95110600789388, auc: 0.7845426057592938\n",
      "Epoch 8/20 \t loss=154.42060664662142 \t val_loss=147.37269439697263, auc: 0.7875372858871839\n",
      "Epoch 9/20 \t loss=153.9635844983553 \t val_loss=148.208740234375, auc: 0.7820042434223774\n",
      "Epoch 10/20 \t loss=153.0344334652549 \t val_loss=148.2769187927246, auc: 0.781793192659067\n",
      "Epoch 11/20 \t loss=152.45655019659745 \t val_loss=148.03406448364257, auc: 0.7836767525918821\n",
      "EARLY STOPPING!!, best AUC: 0.7875372858871839\n",
      "****************************************************************************************************\n",
      "train: 537\n",
      "537 10614 394211\n",
      "train & valid:  16982 4246\n",
      "Epoch 1/20 \t loss=164.12229742932675 \t val_loss=149.6275491153493, auc: 0.8022643173797419\n",
      "Epoch 2/20 \t loss=155.1547853839931 \t val_loss=146.35493155086743, auc: 0.8230732950217023\n",
      "Epoch 3/20 \t loss=153.2457060173376 \t val_loss=147.23872061336743, auc: 0.831055113091451\n",
      "Epoch 4/20 \t loss=152.66445985480917 \t val_loss=144.13718593821807, auc: 0.831226673040524\n",
      "Epoch 5/20 \t loss=151.29251457328223 \t val_loss=144.49900189568015, auc: 0.8257188339037158\n",
      "Epoch 6/20 \t loss=150.55103404486363 \t val_loss=144.14130940156824, auc: 0.827168133087602\n",
      "Epoch 7/20 \t loss=150.05810398841976 \t val_loss=143.61245413387525, auc: 0.8306456749428647\n",
      "EARLY STOPPING!!, best AUC: 0.831226673040524\n",
      "****************************************************************************************************\n",
      "train: 539\n",
      "539 14460 390365\n",
      "train & valid:  23136 5784\n",
      "Epoch 1/20 \t loss=164.98412276886313 \t val_loss=156.86461141835088, auc: 0.7651434869509307\n",
      "Epoch 2/20 \t loss=157.90616917872157 \t val_loss=156.00067039158034, auc: 0.7831008014398511\n",
      "Epoch 3/20 \t loss=156.60000673231193 \t val_loss=154.54463892397672, auc: 0.7791055913518781\n",
      "Epoch 4/20 \t loss=155.87974896273766 \t val_loss=154.6095378709876, auc: 0.7830054875616589\n",
      "Epoch 5/20 \t loss=155.26206219851312 \t val_loss=154.04612765104875, auc: 0.7829120320043621\n",
      "EARLY STOPPING!!, best AUC: 0.7831008014398511\n",
      "****************************************************************************************************\n",
      "train: 629\n",
      "629 4070 400755\n",
      "train & valid:  6512 1628\n",
      "Epoch 1/20 \t loss=165.3574189406175 \t val_loss=148.81753758021762, auc: 0.7971178835130672\n",
      "Epoch 2/20 \t loss=153.67048498300406 \t val_loss=145.06148311070032, auc: 0.8259360123384826\n",
      "Epoch 3/20 \t loss=149.22303023705115 \t val_loss=143.22966766357422, auc: 0.8331633148741863\n",
      "Epoch 4/20 \t loss=146.88125551663907 \t val_loss=141.9205965314593, auc: 0.8382653866705527\n",
      "Epoch 5/20 \t loss=145.43641075721155 \t val_loss=141.4182074410575, auc: 0.8404505792771788\n",
      "Epoch 6/20 \t loss=144.36634107736444 \t val_loss=140.52928379603796, auc: 0.8430365296803652\n",
      "Epoch 7/20 \t loss=143.7445904658391 \t val_loss=140.6022823878697, auc: 0.8462737115029632\n",
      "Epoch 8/20 \t loss=142.92977201021634 \t val_loss=140.17531095232283, auc: 0.8463260832604682\n",
      "Epoch 9/20 \t loss=142.63758615347055 \t val_loss=140.03703580583843, auc: 0.8461568238122996\n",
      "Epoch 10/20 \t loss=142.00723398648776 \t val_loss=139.85428128923687, auc: 0.847781107305936\n",
      "Epoch 11/20 \t loss=141.27451999370868 \t val_loss=140.04541669573103, auc: 0.8445773219663849\n",
      "Epoch 12/20 \t loss=140.81567529531623 \t val_loss=139.53326307024275, auc: 0.8494577625570775\n",
      "Epoch 13/20 \t loss=140.55690266535834 \t val_loss=139.48290906633648, auc: 0.8469924948994463\n",
      "Epoch 14/20 \t loss=140.1333929208609 \t val_loss=139.8201288495745, auc: 0.8483412574079472\n",
      "Epoch 15/20 \t loss=139.63752878629248 \t val_loss=139.7306000845773, auc: 0.8485256970756824\n",
      "EARLY STOPPING!!, best AUC: 0.8494577625570775\n",
      "****************************************************************************************************\n",
      "train: 768\n",
      "768 39411 365414\n",
      "train & valid:  63057 15765\n",
      "Epoch 1/20 \t loss=170.1770857065795 \t val_loss=168.34604780135612, auc: 0.7014079543202775\n",
      "Epoch 2/20 \t loss=168.11286900014534 \t val_loss=167.5840663294638, auc: 0.7079100194333079\n",
      "Epoch 3/20 \t loss=167.4858344074203 \t val_loss=167.68148299186458, auc: 0.7069483858129477\n",
      "Epoch 4/20 \t loss=166.9472312309481 \t val_loss=167.53709805396298, auc: 0.7098105098696748\n",
      "Epoch 5/20 \t loss=166.3660464730824 \t val_loss=167.5488214800435, auc: 0.7078563180246693\n",
      "Epoch 6/20 \t loss=165.7016893919663 \t val_loss=167.8194294591104, auc: 0.7093471319277272\n",
      "Epoch 7/20 \t loss=164.91064284784105 \t val_loss=167.80202717934878, auc: 0.7074312154195534\n",
      "EARLY STOPPING!!, best AUC: 0.7098105098696748\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for target in target_category_str:\n",
    "    score = train_fn(target)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7751066847864541\n"
     ]
    }
   ],
   "source": [
    "print(sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_loaderは全モデルで同じ、違うのはモデルだけ\n",
    "# test_X = pd.read_csv('../output/test_df.csv')\n",
    "test_X = torch.tensor(test_X[feature_cols].values, dtype=torch.float32).to(DEVICE)\n",
    "test = torch.utils.data.TensorDataset(test_X)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "def inference_fn(target_col):\n",
    "    model = make_model(len(feature_cols))\n",
    "    model.load_state_dict(torch.load(f'../nn_models/best_model_{target_col}.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    test_preds = np.zeros(len(test_X))\n",
    "\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        if len(x_batch) == BATCH_SIZE:\n",
    "            test_preds[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = y_pred.cpu().numpy()[:, 0]\n",
    "        else:\n",
    "            test_preds[i * BATCH_SIZE: i * BATCH_SIZE + len(x_batch)] = y_pred.cpu().numpy()[:, 0]\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "110\n",
      "113\n",
      "114\n",
      "134\n",
      "171\n",
      "172\n",
      "173\n",
      "376\n",
      "435\n",
      "467\n",
      "537\n",
      "539\n",
      "629\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(os.path.join(input_path, 'test.csv'))\n",
    "\n",
    "for target in target_category_str:\n",
    "    print(target)\n",
    "    test_df[target] = inference_fn(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>38</th>\n",
       "      <th>110</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>134</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>376</th>\n",
       "      <th>435</th>\n",
       "      <th>467</th>\n",
       "      <th>537</th>\n",
       "      <th>539</th>\n",
       "      <th>629</th>\n",
       "      <th>768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>663721</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.796334</td>\n",
       "      <td>0.945173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969481</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>0.017106</td>\n",
       "      <td>0.973542</td>\n",
       "      <td>0.995672</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999674</td>\n",
       "      <td>0.940650</td>\n",
       "      <td>0.616759</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.958480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>663725</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.964403</td>\n",
       "      <td>0.999663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.012228</td>\n",
       "      <td>0.330852</td>\n",
       "      <td>0.490486</td>\n",
       "      <td>0.994639</td>\n",
       "      <td>0.998270</td>\n",
       "      <td>0.988742</td>\n",
       "      <td>0.918959</td>\n",
       "      <td>0.510049</td>\n",
       "      <td>0.536805</td>\n",
       "      <td>0.681567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>663737</td>\n",
       "      <td>0.991529</td>\n",
       "      <td>0.997166</td>\n",
       "      <td>0.991844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>0.926306</td>\n",
       "      <td>0.284051</td>\n",
       "      <td>0.961448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999856</td>\n",
       "      <td>0.988044</td>\n",
       "      <td>0.976574</td>\n",
       "      <td>0.482622</td>\n",
       "      <td>0.999262</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>663745</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.688305</td>\n",
       "      <td>0.995051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.263248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999599</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.969753</td>\n",
       "      <td>0.918190</td>\n",
       "      <td>0.716128</td>\n",
       "      <td>0.941827</td>\n",
       "      <td>0.999755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>663747</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.866976</td>\n",
       "      <td>0.933117</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.016113</td>\n",
       "      <td>0.598705</td>\n",
       "      <td>0.846999</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.999796</td>\n",
       "      <td>0.993398</td>\n",
       "      <td>0.981169</td>\n",
       "      <td>0.686514</td>\n",
       "      <td>0.029819</td>\n",
       "      <td>0.435119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id        38       110       113       114       134       171  \\\n",
       "0      663721  0.001605  0.796334  0.945173  1.000000  0.969481  0.009845   \n",
       "1      663725  0.008466  0.964403  0.999663  1.000000  0.999951  0.012228   \n",
       "2      663737  0.991529  0.997166  0.991844  1.000000  0.999969  0.926306   \n",
       "3      663745  0.000013  0.688305  0.995051  1.000000  0.978043  1.000000   \n",
       "4      663747  0.000042  0.866976  0.933117  0.999994  0.999946  0.016113   \n",
       "\n",
       "        172       173       376       435       467       537       539  \\\n",
       "0  0.017106  0.973542  0.995672  0.999963  0.999674  0.940650  0.616759   \n",
       "1  0.330852  0.490486  0.994639  0.998270  0.988742  0.918959  0.510049   \n",
       "2  0.284051  0.961448  1.000000  0.999856  0.988044  0.976574  0.482622   \n",
       "3  0.263248  1.000000  0.999599  0.999985  0.969753  0.918190  0.716128   \n",
       "4  0.598705  0.846999  0.999975  0.999796  0.993398  0.981169  0.686514   \n",
       "\n",
       "        629       768  \n",
       "0  0.999738  0.958480  \n",
       "1  0.536805  0.681567  \n",
       "2  0.999262  0.999997  \n",
       "3  0.941827  0.999755  \n",
       "4  0.029819  0.435119  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop('session_id', axis=1, inplace=True)\n",
    "\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%m%d_%H%M\")\n",
    "# test_df.round(5).to_csv(f'../output/submission_{now}_cat.csv', index=None)\n",
    "test_df.round(5).to_csv(f'../pred/nn_1024.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch LSTM with minimum features\n",
    "\n",
    "系列を扱うニューラルネットワークをPyTorchで実装しました。以下の4種の特徴量を入力としています。\n",
    "\n",
    "- CATEGORICAL_COLS: 系列として扱うカテゴリ変数\n",
    "- NUMERICAL_COLS: 系列として扱う数値変数\n",
    "- META_C_COLS: 付随するカテゴリ変数\n",
    "- META_N_COLS: 付随する数値変数\n",
    "\n",
    "```python\n",
    "CATEGORICAL_COLS = [\n",
    "    'value_1',\n",
    "    'display_action_id',\n",
    "    'name_1',\n",
    "    'kind_1'\n",
    "]\n",
    "NUMERICAL_COLS = [\n",
    "    'unit_price',\n",
    "    'n_items',\n",
    "    'spend_time',\n",
    "    'hour'\n",
    "]\n",
    "META_C_COLS = [\n",
    "    'gender',\n",
    "    'age',\n",
    "    'dow',\n",
    "]\n",
    "META_N_COLS = [\n",
    "    'n_seq'    # 予測時点でのsessionの長さ\n",
    "]\n",
    "```\n",
    "\n",
    "1 fold 分を 1 epoch 学習させたところ、検証用データセットの auc が 0.54 でした。\n",
    "\n",
    "```bash\n",
    "Training fold0...\n",
    "1/1 * Epoch (train): 100% 1586/1586 [50:45<00:00,  1.92s/it]\n",
    "1/1 * Epoch (valid): 100% 397/397 [06:18<00:00,  1.05it/s]\n",
    "[2021-01-31 23:12:17,948] \n",
    "1/1 * Epoch 1 (train): auc=0.5298 | loss=0.2419\n",
    "1/1 * Epoch 1 (valid): auc=0.5418 | loss=0.2296\n",
    "Top best models:\n",
    "logdir_nn000/fold0/checkpoints/train.1.pth\t0.2296\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "\n",
    "# https://www.guruguru.science/competitions/14/discussions/9768ddc3-b6f4-440d-830d-cd5330fe1611/\n",
    "class RetailDataset:\n",
    "    def __init__(self, file_path: pathlib.Path, thres_sec: int) -> None:\n",
    "        self.file_path = file_path\n",
    "        self.thres_sec = thres_sec\n",
    "        self.cartlog: pd.DataFrame = pd.read_csv(file_path / \"carlog.csv\",\n",
    "                                                 dtype={'value_1': str},\n",
    "                                                 parse_dates=['date'])\n",
    "        self.product_master: pd.DataFrame = pd.read_csv(\n",
    "            file_path / \"product_master.csv\"\n",
    "        )\n",
    "        self.meta: pd.DataFrame = pd.read_csv(file_path / \"meta.csv\")\n",
    "        self.meta['time_elapsed_sec'] = self.meta['time_elapsed'] * 60\n",
    "        self.test: pd.DataFrame = pd.read_csv(file_path / \"test.csv\")\n",
    "        self.user_master: pd.DataFrame = pd.read_csv(file_path / \"user_master.csv\")\n",
    "        self.target_category_ids = [\n",
    "            38,  # アイスクリーム__ノベルティー\n",
    "            110,  # スナック・キャンディー__ガム\n",
    "            113,  # スナック・キャンディー__シリアル\n",
    "            114,  # スナック・キャンディー__スナック\n",
    "            134,  # チョコ・ビスクラ__チョコレート\n",
    "            171,  # ビール系__RTD\n",
    "            172,  # ビール系__ノンアルコール\n",
    "            173,  # ビール系__ビール系\n",
    "            376,  # 和菓子__米菓\n",
    "            435,  # 大型PET__無糖茶（大型PET）\n",
    "            467,  # 小型PET__コーヒー（小型PET）\n",
    "            537,  # 水・炭酸水__大型PET（炭酸水）\n",
    "            539,  # 水・炭酸水__小型PET（炭酸水）\n",
    "            629,  # 缶飲料__コーヒー（缶）\n",
    "            768,  # 麺類__カップ麺\n",
    "        ]\n",
    "        \"\"\"学習用データセットでは `time_elapsed_sec` が欠損しているので、正解ラベルのために補完\n",
    "        10.0    16833\n",
    "        0.0     14277\n",
    "        5.0     14072\n",
    "        3.0     11304\n",
    "        \"\"\"\n",
    "        self.meta.loc[\n",
    "            self.meta[\"time_elapsed_sec\"].isnull(), \"time_elapsed_sec\"\n",
    "        ] = thres_sec\n",
    "\n",
    "    def get_test_sessions(self) -> set:\n",
    "        \"\"\"以下の条件を満たすセッションを取得する\n",
    "        - 予測対象である\n",
    "        \"\"\"\n",
    "        return set(self.test[\"session_id\"].unique())\n",
    "\n",
    "    def get_test_input_log(self) -> pd.DataFrame:\n",
    "        \"\"\"以下の条件を満たすログを取得する\n",
    "        - 予測対象である\n",
    "\n",
    "        ログが存在しないセッションもあるので注意.\n",
    "        \"\"\"\n",
    "        test_sessions = self.get_test_sessions()\n",
    "        return self.cartlog[self.cartlog[\"session_id\"].isin(test_sessions)]\n",
    "\n",
    "    def get_log_first_half(self) -> pd.DataFrame:\n",
    "        \"\"\"以下の条件を満たすログを取得する\n",
    "        - 学習期間(2020-08-01の前日まで)のセッションである\n",
    "        \"\"\"\n",
    "        first_half_sessions = set(\n",
    "            self.meta.query(\"date < '2020-08-01'\")[\"session_id\"].unique()\n",
    "        )\n",
    "        return self.cartlog[self.cartlog[\"session_id\"].isin(first_half_sessions)]\n",
    "\n",
    "    def get_train_output_log(self) -> pd.DataFrame:\n",
    "        \"\"\"以下の条件を満たすログを取得する\n",
    "        - 学習期間(2020-08-01の前日まで)のセッションである\n",
    "        - 指定した時間(thres_sec)以降にログが存在している\n",
    "        \"\"\"\n",
    "        return pd.merge(\n",
    "            self.get_log_first_half(),\n",
    "            self.meta[[\"session_id\", \"time_elapsed_sec\"]],\n",
    "            on=[\"session_id\"],\n",
    "            how=\"inner\",\n",
    "        ).query(\"spend_time > time_elapsed_sec\")\n",
    "\n",
    "    def get_train_sessions(self) -> set:\n",
    "        \"\"\"以下の条件を満たすセッションを取得する\n",
    "        - 学習期間(2020-08-01の前日まで)のセッションである\n",
    "        - 指定した時間(thres_sec)以降にログが存在している\n",
    "        \"\"\"\n",
    "        return set(self.get_train_output_log()[\"session_id\"].unique())\n",
    "\n",
    "    def get_train_input_log(self) -> pd.DataFrame:\n",
    "        \"\"\"以下の条件を満たすログを取得する\n",
    "        - 学習期間(2020-08-01の前日まで)のセッションである\n",
    "        - 指定した時間(thres_sec)以降にログが存在している\n",
    "        - 指定した時間(thres_sec)より前のログである\n",
    "        \"\"\"\n",
    "        train_sessions = self.get_train_sessions()\n",
    "        return pd.merge(\n",
    "            self.get_log_first_half()[\n",
    "                self.get_log_first_half()[\"session_id\"].isin(train_sessions)\n",
    "            ],\n",
    "            self.meta[[\"session_id\", \"time_elapsed_sec\"]],\n",
    "            on=[\"session_id\"],\n",
    "            how=\"inner\",\n",
    "        ).query(\"spend_time <= time_elapsed_sec\").drop('time_elapsed_sec', axis=1)\n",
    "\n",
    "    def get_payment_sessions(self) -> set:\n",
    "        \"\"\"以下の条件を満たすセッションを取得する\n",
    "        - 決済を行った\n",
    "        \"\"\"\n",
    "        return set(self.cartlog.query(\"is_payment == 1\")[\"session_id\"].unique())\n",
    "\n",
    "    def agg_payment(self, cartlog) -> pd.DataFrame:\n",
    "        \"\"\"セッションごと・商品ごとの購買個数を集計する\"\"\"\n",
    "        # 購買情報は商品のものだけ.\n",
    "        target_index = (cartlog[\"kind_1\"] == \"商品\")\n",
    "\n",
    "        # JANコード (vale_1)ごとに商品の購入個数(n_items)を足し算\n",
    "        agg = (\n",
    "            cartlog.loc[target_index]\n",
    "            .groupby([\"session_id\", \"value_1\"])[\"n_items\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "        agg = agg.rename(columns={\"value_1\": \"JAN\"})\n",
    "        agg = agg.astype({\"JAN\": int})\n",
    "        return agg\n",
    "\n",
    "    def get_train_target(self) -> pd.DataFrame:\n",
    "        \"\"\"学習で使用するセッションの目的変数を取得する\"\"\"\n",
    "        # 空のターゲット用データフレームを用意する\n",
    "        train_sessions = self.get_train_sessions()\n",
    "        train_target = pd.DataFrame(\n",
    "            np.zeros((len(train_sessions), len(self.target_category_ids))),\n",
    "            index=train_sessions,\n",
    "            columns=self.target_category_ids,\n",
    "        ).astype(int)\n",
    "        train_target.index.name = \"session_id\"\n",
    "\n",
    "        # 集計する\n",
    "        train_output_log = self.get_train_output_log()\n",
    "        train_items_per_session_jan = self.agg_payment(train_output_log)\n",
    "        train_items_per_session_target_jan = pd.merge(\n",
    "            train_items_per_session_jan,\n",
    "            self.product_master[[\"JAN\", \"category_id\"]],\n",
    "            on=\"JAN\",\n",
    "            how=\"inner\",\n",
    "        ).query(\"category_id in @self.target_category_ids\")\n",
    "        train_target_pos = (\n",
    "            train_items_per_session_target_jan.groupby([\"session_id\", \"category_id\"])[\n",
    "                \"n_items\"\n",
    "            ]\n",
    "            .sum()\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        train_target_pos[train_target_pos > 0] = 1\n",
    "        train_target_pos[train_target_pos <= 0] = 0\n",
    "\n",
    "        train_target.loc[train_target_pos.index] = train_target_pos.values\n",
    "        return train_target[self.target_category_ids]\n",
    "\n",
    "\n",
    "class RetailNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 input_log_df: pd.DataFrame,\n",
    "                 target_df=None,\n",
    "                 categorical_features=[],\n",
    "                 numerical_features=[],\n",
    "                 meta_c_features=[],\n",
    "                 meta_n_features=[],\n",
    "                 is_train: bool = True) -> None:\n",
    "        super().__init__\n",
    "        self.is_train = is_train\n",
    "        self.input_log_df = input_log_df\n",
    "        self.target_df = target_df\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.meta_c_features = meta_c_features\n",
    "        self.meta_n_features = meta_n_features\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_log_df)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        items = []\n",
    "        for c in self.categorical_features + self.numerical_features:\n",
    "            items.append(self.input_log_df[c].values[index])\n",
    "        meta_c = self.input_log_df[self.meta_c_features].iloc[index].values\n",
    "        meta_n = self.input_log_df[self.meta_n_features].iloc[index].values\n",
    "        items.append(meta_c)\n",
    "        items.append(meta_n)\n",
    "        if self.is_train:\n",
    "            targets = self.target_df[self.target_df.index == self.input_log_df['session_id'].values[index]].values[0]\n",
    "            items.append(targets)\n",
    "        return tuple(items)\n",
    "\n",
    "\n",
    "class MyCollator(object):\n",
    "    def __init__(self, is_train=True):\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        value_1_tensor = [item[0] for item in batch]\n",
    "        display_action_id_tensor = [item[1] for item in batch]\n",
    "        name_1_tensor = [item[2] for item in batch]\n",
    "        kind_1_tensor = [item[3] for item in batch]\n",
    "        unit_price_tensor = [item[4] for item in batch]\n",
    "        n_items_tensor = [item[5] for item in batch]\n",
    "        spend_time_tensor = [item[6] for item in batch]\n",
    "        hour_tensor = [item[7] for item in batch]\n",
    "        meta_c_tensor = [item[8] for item in batch]\n",
    "        meta_n_tensor = [item[9] for item in batch]\n",
    "        if self.is_train:\n",
    "            targets = [item[-1] for item in batch]\n",
    "\n",
    "        def _pad_sequences(data, maxlen: int, dtype=torch.long) -> torch.tensor:\n",
    "            data = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "            return torch.tensor(data, dtype=dtype)\n",
    "\n",
    "        lens = [len(s) for s in value_1_tensor]\n",
    "        value_1_tensor = _pad_sequences(value_1_tensor, max(lens))\n",
    "        display_action_id_tensor = _pad_sequences(display_action_id_tensor, max(lens))\n",
    "        name_1_tensor = _pad_sequences(name_1_tensor, max(lens))\n",
    "        kind_1_tensor = _pad_sequences(kind_1_tensor, max(lens))\n",
    "        unit_price_tensor = _pad_sequences(unit_price_tensor, max(lens), dtype=torch.float)\n",
    "        n_items_tensor = _pad_sequences(n_items_tensor, max(lens), dtype=torch.float)\n",
    "        spend_time_tensor = _pad_sequences(spend_time_tensor, max(lens), dtype=torch.float)\n",
    "        hour_tensor = _pad_sequences(hour_tensor, max(lens), dtype=torch.float)\n",
    "        meta_c_tensor = torch.tensor(meta_c_tensor, dtype=torch.long)\n",
    "        meta_n_tensor = torch.tensor(meta_n_tensor, dtype=torch.float)\n",
    "        if self.is_train:\n",
    "            targets = torch.tensor(targets, dtype=torch.float)\n",
    "            return (\n",
    "                value_1_tensor,\n",
    "                display_action_id_tensor,\n",
    "                name_1_tensor,\n",
    "                kind_1_tensor,\n",
    "                unit_price_tensor,\n",
    "                n_items_tensor,\n",
    "                spend_time_tensor,\n",
    "                hour_tensor,\n",
    "                meta_c_tensor,\n",
    "                meta_n_tensor,\n",
    "                targets,\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            value_1_tensor,\n",
    "            display_action_id_tensor,\n",
    "            name_1_tensor,\n",
    "            kind_1_tensor,\n",
    "            unit_price_tensor,\n",
    "            n_items_tensor,\n",
    "            spend_time_tensor,\n",
    "            hour_tensor,\n",
    "            meta_c_tensor,\n",
    "            meta_n_tensor,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class RetailNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        numerical_features,\n",
    "        meta_c_features,\n",
    "        meta_n_features,\n",
    "        n_targets,\n",
    "        n_value_1,\n",
    "        n_display_action_id,\n",
    "        n_name_1,\n",
    "        n_kind_1,\n",
    "        emb_dim=128,\n",
    "        rnn_dim=128,\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.3,\n",
    "        rnn_dropout=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_value_1 = n_value_1\n",
    "        self.n_targets = n_targets\n",
    "        self.emb_dim = emb_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.rnn_dropout = rnn_dropout\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.meta_c_features = meta_c_features\n",
    "        self.meta_n_features = meta_n_features\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.value_1_embedding = nn.Embedding(n_value_1, emb_dim, padding_idx=0)\n",
    "        self.display_action_id_embedding = nn.Embedding(n_display_action_id, emb_dim, padding_idx=0)\n",
    "        self.name_1_embedding = nn.Embedding(n_name_1, emb_dim, padding_idx=0)\n",
    "        self.kind_1_embedding = nn.Embedding(n_kind_1, emb_dim, padding_idx=0)\n",
    "\n",
    "        self.gender_embedding = nn.Embedding(4, 5)\n",
    "        self.age_embedding = nn.Embedding(12, 10)\n",
    "        self.dow_embedding = nn.Embedding(7, 10)\n",
    "\n",
    "        self.cate_proj = nn.Sequential(\n",
    "            nn.Linear(emb_dim * len(self.categorical_features), hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "        )\n",
    "        self.cont_emb = nn.Sequential(\n",
    "            nn.Linear(len(self.numerical_features), hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=rnn_dropout,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 + 26, hidden_size),\n",
    "#             nn.LayerNorm(hidden_size),\n",
    "#             nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, self.n_targets),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        value_1_tensor,\n",
    "        display_action_id_tensor,\n",
    "        name_1_tensor,\n",
    "        kind_1_tensor,\n",
    "        unit_price_tensor,\n",
    "        spend_time_tensor,\n",
    "        hour_tensor,\n",
    "        n_items_tensor,\n",
    "        meta_c_tensor,\n",
    "        meta_n_tensor,\n",
    "    ):\n",
    "        mc = torch.cat([\n",
    "            self.gender_embedding(meta_c_tensor[:, 0]),\n",
    "            self.age_embedding(meta_c_tensor[:, 1]),\n",
    "            self.dow_embedding(meta_c_tensor[:, 2]),\n",
    "        ], axis=1)\n",
    "\n",
    "        unit_price_tensor_feature = unit_price_tensor.unsqueeze(2)\n",
    "        n_items_tensor_feature = n_items_tensor.unsqueeze(2)\n",
    "        spend_time_tensor_feature = spend_time_tensor.unsqueeze(2)\n",
    "        hour_tensor_feature = hour_tensor.unsqueeze(2)\n",
    "        cate_emb = torch.cat(\n",
    "            [\n",
    "                self.value_1_embedding(value_1_tensor),\n",
    "                self.display_action_id_embedding(display_action_id_tensor),\n",
    "                self.name_1_embedding(name_1_tensor),\n",
    "                self.kind_1_embedding(kind_1_tensor),\n",
    "            ],\n",
    "            dim=2,\n",
    "        )\n",
    "        cate_emb = self.cate_proj(cate_emb)\n",
    "\n",
    "        cont_emb = torch.cat(\n",
    "            [\n",
    "                unit_price_tensor_feature,\n",
    "                n_items_tensor_feature,\n",
    "                spend_time_tensor_feature,\n",
    "                hour_tensor_feature,\n",
    "            ],\n",
    "            dim=2,\n",
    "        )\n",
    "        cont_emb = self.cont_emb(cont_emb)\n",
    "        out = torch.cat([cate_emb, cont_emb], dim=2)\n",
    "        out, _ = self.lstm(out)\n",
    "        avg_pool = torch.mean(out, 1)\n",
    "        max_pool, _ = torch.max(out, 1)\n",
    "        conc = torch.cat([avg_pool, max_pool, mc, meta_n_tensor[:, ]], axis=1)\n",
    "        conc = self.ffn(conc)\n",
    "        return conc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catalyst.dl import Runner\n",
    "from catalyst.dl.utils import any2device\n",
    "\n",
    "\n",
    "class CustomRunner(Runner):\n",
    "    def _handle_batch(self, batch):\n",
    "        (\n",
    "            value_1_tensor,\n",
    "            display_action_id_tensor,\n",
    "            name_1_tensor,\n",
    "            kind_1_tensor,\n",
    "            unit_price_tensor,\n",
    "            n_items_tensor,\n",
    "            spend_time_tensor,\n",
    "            hour_tensor,\n",
    "            meta_c_tensor,\n",
    "            meta_n_tensor,\n",
    "            y,\n",
    "        ) = batch\n",
    "        out = self.model(\n",
    "            value_1_tensor,\n",
    "            display_action_id_tensor,\n",
    "            name_1_tensor,\n",
    "            kind_1_tensor,\n",
    "            unit_price_tensor,\n",
    "            n_items_tensor,\n",
    "            spend_time_tensor,\n",
    "            hour_tensor,\n",
    "            meta_c_tensor,\n",
    "            meta_n_tensor,\n",
    "        )\n",
    "        loss = self.criterion(out, y)\n",
    "        try:\n",
    "            score = roc_auc_score(y.to('cpu').detach().numpy().copy(), out.to('cpu').detach().numpy().copy(), average='macro')\n",
    "            self.batch_metrics.update(\n",
    "                {\"loss\": loss, \"auc\": score}\n",
    "            )\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_batch(self, batch):\n",
    "        batch = any2device(batch, self.device)\n",
    "        if len(batch) == 10:\n",
    "            (\n",
    "                value_1_tensor,\n",
    "                display_action_id_tensor,\n",
    "                name_1_tensor,\n",
    "                kind_1_tensor,\n",
    "                unit_price_tensor,\n",
    "                n_items_tensor,\n",
    "                spend_time_tensor,\n",
    "                hour_tensor,\n",
    "                meta_c_tensor,\n",
    "                meta_n_tensor,\n",
    "            ) = batch\n",
    "        elif len(batch) == 11:\n",
    "            (\n",
    "                value_1_tensor,\n",
    "                display_action_id_tensor,\n",
    "                name_1_tensor,\n",
    "                kind_1_tensor,\n",
    "                unit_price_tensor,\n",
    "                n_items_tensor,\n",
    "                spend_time_tensor,\n",
    "                hour_tensor,\n",
    "                meta_c_tensor,\n",
    "                meta_n_tensor,\n",
    "                y,\n",
    "            ) = batch\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "        out = self.model(\n",
    "            value_1_tensor,\n",
    "            display_action_id_tensor,\n",
    "            name_1_tensor,\n",
    "            kind_1_tensor,\n",
    "            unit_price_tensor,\n",
    "            n_items_tensor,\n",
    "            spend_time_tensor,\n",
    "            hour_tensor,\n",
    "            meta_c_tensor,\n",
    "            meta_n_tensor,\n",
    "        )\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# sys.path.append('../')\n",
    "# from src.datasets import RetailNNDataset, MyCollator\n",
    "# from src.models import RetailNN\n",
    "# from src.utils import seed_everything\n",
    "# from src.runner import CustomRunner\n",
    "\n",
    "CATEGORICAL_COLS = [\n",
    "    'value_1',\n",
    "    'display_action_id',\n",
    "    'name_1',\n",
    "    'kind_1'\n",
    "]\n",
    "NUMERICAL_COLS = [\n",
    "    'unit_price',\n",
    "    'n_items',\n",
    "    'spend_time',\n",
    "    'hour'\n",
    "]\n",
    "META_C_COLS = [\n",
    "    'gender',\n",
    "    'age',\n",
    "    'dow',\n",
    "]\n",
    "META_N_COLS = [\n",
    "    'n_seq'\n",
    "]\n",
    "run_name = 'nn000'\n",
    "seed_everything(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('File loading...')\n",
    "    X_train = pd.read_pickle('../input/atmacup9-features/X_train.pickle')\n",
    "    y_train = pd.read_pickle('../input/atmacup9-features/y_train.pickle')\n",
    "    X_test = pd.read_pickle('../input/atmacup9-features/X_test.pickle')\n",
    "\n",
    "    cv = MultilabelStratifiedKFold(n_splits=5, shuffle=False)\n",
    "    oof_preds = np.zeros((len(X_train), 15), dtype=np.float32)\n",
    "    test_preds = np.zeros((len(X_test), 15), dtype=np.float32)\n",
    "    cv_scores = []\n",
    "\n",
    "    test_dataset = RetailNNDataset(input_log_df=X_test,\n",
    "                                   target_df=None,\n",
    "                                   categorical_features=CATEGORICAL_COLS,\n",
    "                                   numerical_features=NUMERICAL_COLS,\n",
    "                                   meta_c_features=META_C_COLS,\n",
    "                                   meta_n_features=META_N_COLS,\n",
    "                                   is_train=False)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=MyCollator(is_train=False),\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=os.cpu_count(),\n",
    "                             pin_memory=True)\n",
    "\n",
    "    for fold_id, (tr_idx, va_idx) in enumerate(cv.split(X_train,\n",
    "                                                        y_train.loc[y_train.index.isin(X_train[\"session_id\"].to_list())])):\n",
    "        if fold_id in (0,):\n",
    "            print(f'Training fold{fold_id}...')\n",
    "            X_tr = X_train.loc[tr_idx, :]\n",
    "            X_val = X_train.loc[va_idx, :]\n",
    "\n",
    "            train_dataset = RetailNNDataset(input_log_df=X_tr,\n",
    "                                            target_df=y_train,\n",
    "                                            categorical_features=CATEGORICAL_COLS,\n",
    "                                            numerical_features=NUMERICAL_COLS,\n",
    "                                            meta_c_features=META_C_COLS,\n",
    "                                            meta_n_features=META_N_COLS)\n",
    "            valid_dataset = RetailNNDataset(input_log_df=X_val,\n",
    "                                            target_df=y_train,\n",
    "                                            categorical_features=CATEGORICAL_COLS,\n",
    "                                            numerical_features=NUMERICAL_COLS,\n",
    "                                            meta_c_features=META_C_COLS,\n",
    "                                            meta_n_features=META_N_COLS)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset,\n",
    "                                      shuffle=False,\n",
    "                                      collate_fn=MyCollator(is_train=True),\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_workers=os.cpu_count(),\n",
    "                                      pin_memory=True)\n",
    "            valid_loader = DataLoader(valid_dataset,\n",
    "                                      shuffle=False,\n",
    "                                      collate_fn=MyCollator(is_train=True),\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_workers=os.cpu_count(),\n",
    "                                      pin_memory=True)\n",
    "\n",
    "            loaders = {'train': train_loader, 'valid': valid_loader}\n",
    "\n",
    "            runner = CustomRunner(device=device)\n",
    "            model = RetailNN(\n",
    "                categorical_features=CATEGORICAL_COLS,\n",
    "                numerical_features=NUMERICAL_COLS,\n",
    "                meta_c_features=META_C_COLS,\n",
    "                meta_n_features=META_N_COLS,\n",
    "                n_targets=15,\n",
    "                n_value_1=67590 + 1 + 1,     # len(classes_) + padding_id + null_id\n",
    "                n_display_action_id=41 + 1 + 1,     # len(classes_) + padding_id + null_id\n",
    "                n_name_1=42070 + 1 + 1,     # len(classes_) + padding_id + null_id\n",
    "                n_kind_1=14 + 1 + 1,     # len(classes_) + padding_id + null_id\n",
    "            )\n",
    "            criterion = torch.nn.BCEWithLogitsLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
    "            logdir = f'logdir_{run_name}/fold{fold_id}'\n",
    "            runner.train(\n",
    "                model=model,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                loaders=loaders,\n",
    "                logdir=logdir,\n",
    "                num_epochs=1,\n",
    "                verbose=True,\n",
    "            )\n",
    "            oof_preds[va_idx, :] = np.concatenate(list(map(lambda x: x.cpu().numpy(),\n",
    "                                                  runner.predict_loader(loader=valid_loader,\n",
    "                                                                        resume=f'{logdir}/checkpoints/best.pth',\n",
    "                                                                        model=model,),)))\n",
    "            np.save(f\"{logdir}/y_val_pred_fold{fold_id}\", oof_preds[va_idx, :])\n",
    "\n",
    "            test_preds_ = np.concatenate(list(map(lambda x: x.cpu().numpy(),\n",
    "                                         runner.predict_loader(loader=test_loader,\n",
    "                                                               resume=f'{logdir}/checkpoints/best.pth',\n",
    "                                                               model=model,),)))\n",
    "            test_preds += test_preds_ / cv.n_splits\n",
    "            np.save(f\"{logdir}/y_test_pred_fold{fold_id}\", test_preds_)\n",
    "\n",
    "    np.save(f\"{logdir}/y_oof_pred\", oof_preds)\n",
    "    np.save(f\"{logdir}/y_test_pred\", test_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     _b = batch\n",
    "#     model = RetailNN(\n",
    "#         categorical_features=CATEGORICAL_COLS,\n",
    "#         numerical_features=NUMERICAL_COLS,\n",
    "#         meta_c_features=META_C_COLS,\n",
    "#         meta_n_features=META_N_COLS,\n",
    "#         n_targets=15,\n",
    "#         n_value_1=67590 + 1 + 1,     # len(classes_) + padding_id + null_id\n",
    "#         n_display_action_id=41 + 1 + 1,     # len(classes_) + padding_id + null_id\n",
    "#         n_name_1=42070 + 1 + 1,     # len(classes_) + padding_id + null_id\n",
    "#         n_kind_1=14 + 1 + 1,     # len(classes_) + padding_id + null_id\n",
    "#     )\n",
    "#     out = model(_b[0], _b[1], _b[2], _b[3], _b[4], _b[5], _b[6], _b[7], _b[8], _b[9])\n",
    "#     print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)\n",
    "\n",
    "# RetailNN(\n",
    "#   (drop): Dropout(p=0.3, inplace=False)\n",
    "#   (value_1_embedding): Embedding(67592, 128, padding_idx=0)\n",
    "#   (display_action_id_embedding): Embedding(43, 128, padding_idx=0)\n",
    "#   (name_1_embedding): Embedding(42072, 128, padding_idx=0)\n",
    "#   (kind_1_embedding): Embedding(16, 128, padding_idx=0)\n",
    "#   (gender_embedding): Embedding(4, 5)\n",
    "#   (age_embedding): Embedding(12, 10)\n",
    "#   (dow_embedding): Embedding(7, 10)\n",
    "#   (cate_proj): Sequential(\n",
    "#     (0): Linear(in_features=512, out_features=64, bias=True)\n",
    "#     (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
    "#   )\n",
    "#   (cont_emb): Sequential(\n",
    "#     (0): Linear(in_features=4, out_features=64, bias=True)\n",
    "#     (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
    "#   )\n",
    "#   (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
    "#   (ffn): Sequential(\n",
    "#     (0): Linear(in_features=282, out_features=128, bias=True)\n",
    "#     (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
    "#     (2): Dropout(p=0.3, inplace=False)\n",
    "#     (3): ReLU(inplace=True)\n",
    "#     (4): Linear(in_features=128, out_features=15, bias=True)\n",
    "#   )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
